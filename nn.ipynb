{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, h, w = train_X.shape\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num examples: 60000\n",
      "height of the image: 28\n",
      "width of the image: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"num examples:\", m)\n",
    "print(\"height of the image:\", h)\n",
    "print(\"width of the image:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook implements a plain NN from scratch, using numpy as the only dependency\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This notebook implements a plain NN from scratch, using numpy as the only dependency.\n",
    "Algo: batch gradient descent.\n",
    "Each layer uses sigmoid activation function. The last layer - softmax activation\n",
    "'''\n",
    "def train_nn(train_X, train_y, layers, num_epochs, learning_rate):\n",
    "    #layers -> an array where len(layers) == num_layers, \n",
    "    #and layers[i] -> number of hidden units in i+1-th layer \n",
    "    parameters = initialize_parameters(layers)#use xavier initialization\n",
    "    for i in range(num_epochs):\n",
    "        cache = for_prop()\n",
    "        d_params = back_prop(cache)#this updates parameters\n",
    "        update_params(d_params)\n",
    "        compute_loss()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_parameters()\n",
    "sigmoid()\n",
    "one_hot_encoding()\n",
    "# sigmoid_derivative()\n",
    "for_prop()\n",
    "# back_prop()\n",
    "# softmax_activation?\n",
    "# compute_loss()\n",
    "# update_params(d_params)\n",
    "# gradient_checking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#given an np array, compute the elementwise sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "import math\n",
    "sigmoid(np.array([math.inf, -math.inf]))\n",
    "sigmoid(np.array([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 associated mat:\n",
      " [[-0.30975372 -0.02570985  0.03452575 -0.0338122  -0.15374349]\n",
      " [ 0.14178843  0.39882064  0.28817877  0.2410015   0.21077359]\n",
      " [-0.17032731 -0.34827163  0.32654291  0.33598577  0.32584743]\n",
      " [ 0.06765052 -0.20619857 -0.53953438 -0.2516183  -0.26522662]]\n",
      "b1 associated mat:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 associated mat:\n",
      " [[-0.17150038  0.12210365 -0.01334078 -0.21464894]\n",
      " [-0.26890502  0.4802588  -0.23172553 -0.25110254]\n",
      " [-0.04777167  0.02828064  0.32422867 -0.50301718]\n",
      " [ 0.30523133  0.20739956  0.10669511  0.05081905]]\n",
      "b2 associated mat:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 associated mat:\n",
      " [[-0.25802778 -0.41020653 -0.09972408 -0.13473209]\n",
      " [ 0.26012368  0.01810268  0.4960611   0.07065452]\n",
      " [-0.43824347 -0.39730782 -0.15219173  0.19756516]]\n",
      "b3 associated mat:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters(layers):\n",
    "    L = len(layers)\n",
    "    parameters = {}\n",
    "    for l in range(L-1):\n",
    "        n_in = layers[l]#number of input units\n",
    "        n_out = layers[l+1]#number of output units\n",
    "        #xavier initialization for W\n",
    "        parameters[\"W\"+str(l+1)] = np.random.randn(n_out, n_in)*np.sqrt(1/(n_out+n_in))\n",
    "        parameters[\"b\"+str(l+1)] = np.zeros((n_out, 1))\n",
    "    return parameters\n",
    "\n",
    "params = initialize_parameters([5,4,4,3])\n",
    "for key in params:\n",
    "    print(key, \"associated mat:\\n\", params[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop_l(A_prev, W, b):\n",
    "    Z = np.dot(W,A_prev)+b\n",
    "    A = sigmoid(Z)\n",
    "    return A\n",
    "    \n",
    "def for_prop(X, parameters, layers):\n",
    "    L = len(layers)\n",
    "    A = X\n",
    "    for l in range(L-1):\n",
    "        W = parameters['W'+str(l+1)]\n",
    "        b = parameters['b'+str(l+1)]\n",
    "        A = for_prop_l(A, W, b)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "[[0.38437521 0.37996516]\n",
      " [0.60776942 0.61237131]\n",
      " [0.40488275 0.39941156]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,1,1,1,1],[2,2,2,2,2]])\n",
    "X = X.T\n",
    "print(X.shape)\n",
    "A = for_prop(X, params,[5,4,4,3])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84203357 0.25      ]\n",
      " [0.04192238 0.25      ]\n",
      " [0.00208719 0.25      ]\n",
      " [0.11395685 0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):#do columnwise-softmax\n",
    "    t = np.exp(x)\n",
    "    return t/t.sum(axis=0)\n",
    "x = np.array([[5,2,-1,3],[1,1,1,1]])\n",
    "x = x.T\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148.4131591    7.3890561    0.36787944  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
